# 线性回归


## 概念

在统计学中，**线性回归**（英语：linear regression）是利用称为线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种[回归分析](https://zh.wikipedia.org/wiki/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90 "回归分析")。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做[多元回归](https://zh.wikipedia.org/wiki/%E4%B8%80%E8%88%AC%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B "一般线性模型")（multivariable linear regression）。

在线性回归中，数据使用线性预测函数来建模，并且未知的模型参数也是通过数据来估计。这些模型被叫做线性模型。最常用的线性回归建模是给定X值的y的条件均值是X的仿射函数。不太一般的情况，线性回归模型可以是一个中位数或一些其他的给定X的条件下y的条件分布的分位数作为X的线性函数表示。像所有形式的回归分析一样，线性回归也把焦点放在给定X值的y的条件概率分布，而不是X和y的联合概率分布（多元分析领域）。

线性回归是回归分析中第一种经过严格研究并在实际应用中广泛使用的类型。这是因为线性依赖于其未知参数的模型比非线性依赖于其未知参数的模型更容易拟合，而且产生的估计的统计特性也更容易确定。

线性回归有很多实际用途。分为以下两大类：

1. 如果目标是预测或者映射，线性回归可以用来对观测数据集的和X的值拟合出一个预测模型。当完成这样一个模型以后，对于一个新增的X值，在没有给定与它相配对的y的情况下，可以用这个拟合过的模型预测出一个y值。
2. 给定一个变量y和一些变量 $X_1,...,X_p$，这些变量有可能与y相关，线性回归分析可以用来量化y与 $X_j$ 之间相关性的强度，评估出与y不相关的 $X_j$，并识别出哪些 $X_j$ 的子集包含了关于y的冗余信息。

线性回归模型经常用最小二乘逼近来拟合，但他们也可能用别的方法来拟合，比如用最小化“拟合缺陷”在一些其他规范里（比如最小绝对误差回归），或者在桥回归中最小化最小二乘损失函数的惩罚。相反，最小二乘逼近可以用来拟合那些非线性的模型。因此，尽管“最小二乘法”和“线性模型”是紧密相连的，但他们是不能划等号的。

## 理论模型

给一个随机样本${\displaystyle (Y_{i},X_{i1},\ldots ,X_{ip}),\,i=1,\ldots ,n}$，一个线性回归模型假设回归子${\displaystyle Y_{i}}$和回归量${\displaystyle X_{i1},\ldots ,X_{ip}}$之间的关系是除了$X$的影响以外，还有其他的变量存在。我们加入一个误差项 ${\displaystyle \varepsilon_{i}}$（也是一个随机变量）来捕获除了${\displaystyle X_{i1},\ldots ,X_{ip}}$之外任何对${\displaystyle Y_{i}}$的影响。所以一个多变量线性回归模型表示为以下的形式：
${\displaystyle Y_{i}=\beta_{0}+\beta_{1}X_{i1}+\beta_{2}X_{i2}+\ldots +\beta_{p}X_{ip}+\varepsilon_{i},\qquad i=1,\ldots ,n}$



其他的模型可能被认定成非线性模型。一个线性回归模型不需要是自变量的线性函数。线性在这里表示${\displaystyle Y_{i}}$的条件均值在参数${\displaystyle \beta }$里是线性的。例如：模型${\displaystyle Y_{i}=\beta_{1}X_{i}+\beta_{2}X_{i}^{2}+\varepsilon_{i}}$在${\displaystyle \beta_{1}}$和${\displaystyle \beta_{2}}$里是线性的，但在${\displaystyle X_{i}^{2}}$里是非线性的，它是${\displaystyle X_{i}}$的非线性函数。

- 单变量数据考虑的是一个单一变量的频数或概率，所描述的对象只有一种

- 二变量数据能够了解不同变量之间的关系

**两个变量之间存在相关关系并不一定意味着一个变量会影响另一个变量，也不意味着二者存在实际关系**

**SSE**: 误差平方和，$SS-E=\sum(y-\hat{y})^2$ 

我们用 $y=ax+b$ 来拟合数据，其中 $a$ 代表这条直线的斜率，使得 $SSE$ 最小的 $a$ 值计算如下：

$$
a=\frac{\sum{((x-\overline{x})(y-\overline{y}))}}{\sum{(x-\overline{x})^2}}
$$

或者

$$
a=\frac{\overline{xy}-\overline{x}\cdot\overline{y}}{\overline{x^2}-\overline{x}^2}
$$

$$
b=\overline{y}-a\overline{x}
$$

### 相关系数

相关系数可以指出数据点与直线的拟合程度，同时能够指出我们的期望预测结果能够达到的精确程度。相关系数是介于-1和1之间的一个数，描述了各个数据点与直线的偏离程度，通过它就可以量度回归线与数据的拟合度，通常用字母$R​$ 表示

如果$R$ 等于-1，则数据为完全负线性相关，所有数据点都在一条直线上；如果$R$ 等于1，则数据完全正线性相关。如果$R$ 等于0，则不存在相关性，$R$越接近0，线性相关性越弱

$$
R=\frac{as_x}{s_y}
$$

$a$是已经求出的最佳拟合线的斜率，$s_x$ 是样本中$x$值的标准差，$s_y$是$y$值的标准差

$$
s_x=\sqrt{\frac{\sum{(x-\overline{x})^2}}{n-1}}
$$

直线的平方误差显示出，总波动中有多少没有被回归线描述

### 决定系数

决定系数（coefficient of determination）:在$Y$的总平方和中，由X引起的平方和所占的比例，记为$R^2 $(R的平方)，有的教材上翻译为判定系数，也称为拟合优度。是相关系数的平方。表示可根据自变量的变异来解释因变量的变异部分。

当$R^ 2$越接近1时，表示相关的方程式参考价值越高；相反，越接近0时，表示参考价值越低。这是在一元回归分析中的情况。但从本质上说决定系数和回归系数没有关系，就像标准差和标准误差在本质上没有关系一样。

意义：拟合优度越大，自变量对因变量的解释程度越高，自变量引起的变动占总变动的百分比高。观察点在回归直线附近越密集。

表达式：$R^2=SSR/SST=1-SSE/SST​$

其中：$SST=SSR+SSE$，$SST$ (total sum of squares)为总平方和，$SSR$ (regression sum of squares)为回归平方和，$SSE$ (error sum of squares) 为残差平方和。

- 回归平方和(已解释方差平方和)：
  
  $SSR(Sum \; of \;  Squares \; for\;  regression) = ESS (explained \; sum \; of \; squares)$ 

- 残差平方和：
  
  $SSE（Sum\;  of \; Squares\;  for\;  Error） = RSS (residual\;  sum\;  of\;  squares) =SSR(sum\;  of \; squared\;  residuals)$

- 总离差平方和：
  
  $SST(Sum \; of \; Squares \; for\;  total) = TSS(total \; sum \; of \; squares)​$
  
  决定系数与相关系数区别

| 判定系数            | 相关系数            |
| --------------- | --------------- |
| 就模型而言           | 就两个变量而言         |
| 说明解释变量对因变量的解释程度 | 度量两个变量线性依存程度    |
| 度量不对称的因果关系      | 度量不含因果关系的对称相关关系 |
| 取值：[0,1]        | 取值：[－1,1]       |

## 最小二乘回归法

最小二乘回归法是一种数学方法，可用一条最佳拟合线将一组二维变量数据拟合，通过将公式$y=ax+b​$ 的一条直线与一组数值相拟合，使得误差平方和最小——即，使得实际数值与这些数值的估计值之间的差值最小。

这条直线称为回归线，在预测一个特定$x$值对应的$y$ 值时，要避免对已知数据点范围以外的值进行预测

## 协方差

协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。

协方差表示的是两个变量的总体的误差，这与只表示一个变量误差的方差不同。 如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。 如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。

期望值分别为$E(X)$与$E(Y)$的两个实随机变量$X$与$Y$之间的协方差$Cov(X,Y)$定义为：




$$
\begin{aligned}
Cov(X,Y)&=E((X-E(X)(Y-E(Y))\\\
&=E(XY)-2E(Y)E(X)+E(X)E(Y)\\\
&=E(XY)-E(X)E(Y)
\end{aligned}
$$

协方差为0的两个随机变量称为是不相关的。

皮尔森相关系数（$Pearson$）

$$
\rho_{XY}=\frac{Cov(X,Y)}{\sqrt {D(X)}\cdot\sqrt{D(Y)}}
$$

## 总结

<img src="https://i0.hdslb.com/bfs/album/852d886b39c81c8066bd3d94e085869c5d169590.png" title="" alt="" data-align="center">
