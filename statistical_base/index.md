# 统计学基础、二项及泊松分布


## 统计学分类

- 描述统计学 descriptive statistics
  
  侧重以数字特征描述数据分布情况

- 推论统计学 Inferential statistics
  
  侧重以数据的子集分布情况来推断数据整体情况

## 集中趋势的量度:广义平均数

在统计学中，[集中趋势]([集中趋势 - 维基百科，自由的百科全书 (wikipedia.org)](https://zh.wikipedia.org/wiki/%E9%9B%86%E4%B8%AD%E8%B6%8B%E5%8A%BF))（central tendency）或**中央趋势**，在口语上也经常被称为**平均**，表示一个机率分布的中间值。 最常见的几种集中趋势包括**算数平均数、中位数及众数**。集中趋势可以由有限的数组（如一群样本）中或理论上的机率分配（如[正态分布](https://baike.baidu.com/item/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83)）中求得。有些人使用集中趋势（或**集中性**）这个词汇以表示“数量化的资料之中央值的趋势”。在这种意义下，我们可以利用资料的[离散程度](https://baike.baidu.com/item/%E7%A6%BB%E6%95%A3%E7%A8%8B%E5%BA%A6)（例如[标准偏差](https://baike.baidu.com/item/%E6%A0%87%E5%87%86%E5%81%8F%E5%B7%AE)或[四分差](https://baike.baidu.com/item/%E5%9B%9B%E5%88%86%E5%B7%AE)等相似的统计量）判别其集中趋势的程度。

- 均值（mean）: 通常指代算术平均数，另外还有几何平均数，调和平均数

- 中位数（medium）： 当数据个数为奇数时，指代中间那个数，否则一般取中间两个数的平均值

- 众数（mode）： 所有数中出现频率最高的数

有时候相较于中位数而言，，均值受到离群值影响较大，不能很好地描述数据的集中趋势。以数据3,3,3,3,100为例，中位数更能代表总体的集中趋势。如果存在异常值，均值会往异常值方向移动

下面是常用的三种集中趋势的统计量（广义平均数，average）

| 平均数 | 计算方法                                              | 使用场景                       |
| --- |:-------------------------------------------------:| -------------------------- |
| 均值  | $\frac{\sum\rm{X}}{n}$                            | 在数据非常对称，且仅显示出一种趋势时使用       |
| 中位数 | 将所有数据按照升序顺序排列，如果有奇数个值，则为中间的数；如果有偶数个值，则为中间两个数的算术平均 | 在数据由于异常值而发生偏斜时使用           |
| 众数  | 选出具有最大频数的一个或几个数值，如果数据可分为两组，则为每组找出一个众数             | 在遇到类别数据时使用，当数据可以分为两个以上组时使用 |

## 样本和总体

可以用样本(Sample)来分析总体(Population)的情况，因为有时候去分析总体的统计量很费时费力。

可以用随机抽样来获取属于总体的样本。

$\mu​$ 一般指代总体均值(population mean)  ，$\mu =\frac{\sum_{i=1}^{N}x_i}{N}$

$\overline{\rm X}​$ 一般指代样本的均值(sample mean)，$\overline{\rm X}=\frac{\sum_{i=1}^{n}x_i}{n}$

## 离中趋势的量度：“矩”

离中趋势(Dispersion)表现为分散性与变异性

### 全矩(极差)

全矩也称极差，适用于量度数据集分散程度的一种方法，其算法为：***上界 -下界***， 其中，上界为最大值，下界为最小值，全矩的测量有点像测量数据的宽度

**全矩仅仅描述了数据的宽度，并没有描述数据在上、下界之间的分布形态，且全矩很容易受到异常值的影响**

### 四分位矩

四分位数是这样一些**数值**，**它们将数据一分为四**，最小的四分位数称为下四分位数，最大的四分位数称为上四分位数，中间的四分位数即中位数

***四分位矩 =上四分位数-下四分位数***

与全矩相比，四分位矩较少受到异常值的影响

同理，将数据分成十份，有十分位数和十分位矩，将数据分成100份，有百分位矩

百分位数：第 $k$ 百分位数即位于数据范围 $k\%$ 处的数值，即为 $\mathrm{P_k}$ 

### 方差

$\sigma^2$ 指代**方差(variance)**

$$
\sigma^2=\frac{\sum_{i=1}^N\left(x_i-\mu\right)^2}{N}
$$

方差可以描述数据离均值的远近程度，方差越小表示数据更加集中

样本方差是用来估计总体方差，为了提供无偏估计，则对应的无偏(unbias)估计样本方差为

$$
s^2=\frac{\sum_{i=1}^n\left(x_i-\overline{\rm X}\right)^2}{n-1}
$$

    

 $\sigma $ 指代**标准差**(**standard variance**)，是方差的平方根

标准差能够在量纲上有更容易理解的表示，它的单位与均值的单位一致，我们平时可以这样描述：该数字里均值相差了几个标准差

方差公式可以简化如下，更加方便计算

$$
\begin{aligned}
\sigma^2 
&=\frac{\sum_{i=1}^N\left(x_i-\mu\right)^2}{N}\\\
&=\frac{\sum_{i=1}^N\left(x_i^2-2x_i\mu+\mu^2\right)}{N}\\\
&=\frac{\sum_{i=1}^Nx_i^2-2\mu\sum_{i=1}^Nx_i+\mu^2\sum_{i=1}^N1}{N}\\\
&=\frac{\sum_{i=1}^Nx_i^2-2\mu\sum_{i=1}^Nx_i+\mu^2N}{N}\\\
&=\frac{\sum_{i=1}^Nx_i^2}{N}-2\mu^2+\mu^2\\\
&=\frac{\sum_{i=1}^Nx_i^2}{N}-\mu^2\\\
&=E\left(\rm X^2\right)-E\left(\rm X\right)^2\\\
\end{aligned}
$$

### 标准分(Z-Score)

使用标准分可以对不同数据集的数据进行比较，而这些不同数据集的均值和标准差各不相同——标准分是对不同环境下的相关数据进行比较的一种方法

通过整个数据集的均值和标准差可以计算出一个特数值的标准分。标准分通常以字母'z'表示，计算方法如下：

$$
z=\frac{x-\mu}{\sigma}
$$

通过计算标准差，我们可以把不同数据集的数值视为来自同一个数据集或数据分布，从而进行比较

标准分的作用是将几个数据集转换成一个理论上的新分布，这个分布的均值为0，标准差为1，这是一种可以用于进行比较的通用分布，即服从标准正态分布

**标准分的含义是距离均值的标准差的个数，即标准分=距离均值的标准差的个数**

Z-Score最大的优点就是简单，容易计算，在R中，不需要加载包，仅仅凭借最简单的数学公式就能够计算出Z-Score并进行比较。此外，Z-Score能够应用于数值型的数据，并且不受数据量级的影响，因为它本身的作用就是消除量级给分析带来的不便。

但是Z-Score应用也有风险。首先，估算Z-Score需要总体的平均值与方差，但是这一值在真实的分析与挖掘中很难得到，大多数情况下是用样本的均值与标准差替代。其次，Z-Score对于数据的分布有一定的要求，正态分布是最有利于Z-Score计算的。最后，Z-Score消除了数据具有的实际意义，A的Z-Score与B的Z-Score与他们各自的分数不再有关系，因此Z-Score的结果只能用于比较数据间的结果，数据的真实意义还需要还原原值。

## 随机变量

随机变量(Random Variable)一般用大写的英文字母表示，可能是为了和传统的变量区分开来，实际上，它更像是一种函数，将随机过程映射到数字，而这个数字是随机的。

- 离散(Discrete)随机变量：取值有限

- 连续(Continuous)随机变量：取值无限

### 数学期望(Expected Value)

期望计算：

$$
E(X)=\sum{xP(X=x)}
$$

方差计算：

$$
Var(X)=E(X-\mu)^2
$$

利用线性变换计算：

$$
\begin{aligned}
E(aX+b)&=aE(X)+b \\\
Var(aX+b)&=a^2Var(X) \\\
E(X_1+X_2+...+X_n)&=nE(X) \\\
Var(X_1+X_2+...+X_n)&=nVar(X) \\\
\end{aligned}
$$

多个随机变量的线性变换：

$$
\begin{aligned}
E(X-Y)=E(X)-E(Y)\\\
E(X+Y)=E(X)+E(Y) \\\
Var(X+Y)=Var(X)+Var(Y)\\\
Var(X-Y)=Var(X)+Var(Y)\\\
\end{aligned}
$$

其中X与Y互相独立，独立随机变量做减法运算，方差依然增大

### 概率密度函数

**概率(probability)**

当我们求取值在某个值附近的概率时，我们会这样表示：$ \rm{P}(|\rm{X}- \textit{value}<0.1|)=0.3$ ,即随机变量$\rm{X}​$ 取值为value附近的概率，精度在0.1之内

对于一维实随机变量*X*，设它的累积分布函数是 $\displaystyle F_{X}(x)$ 。如果存在可测函数 $\displaystyle f_{X}(x)$，满足

$$
\begin{aligned}
 \forall -\infty <a<\infty ,\quad F_{X}(a)=\int_{-\infty }^{a}f_{X}(x)dx
\end{aligned}
$$

其中$\displaystyle f_{X}(x)$ 表示概率密度函数,${\displaystyle F_{X}(x)}$ 表示概率分布函数

## 二项分布

### 二项分布定义

在概率论和统计学中，**二项分布**（Binomial distribution）是*n*个[独立](https://zh.wikipedia.org/wiki/%E7%B5%B1%E8%A8%88%E7%8D%A8%E7%AB%8B%E6%80%A7 "统计独立性")的是/非试验中成功的次数的[离散概率分布](https://zh.wikipedia.org/wiki/%E7%A6%BB%E6%95%A3%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83 "离散概率分布")，其中每次试验的成功概率为*p*。这样的单次成功/失败试验又称为[伯努利试验](https://zh.wikipedia.org/wiki/%E4%BC%AF%E5%8A%AA%E5%88%A9%E8%A9%A6%E9%A9%97 "伯努利试验")。实际上，当*n* = 1时，二项分布就是[伯努利分布](https://zh.wikipedia.org/wiki/%E4%BC%AF%E5%8A%AA%E5%88%A9%E5%88%86%E5%B8%83 "伯努利分布")。二项分布是[显著性差异](https://zh.wikipedia.org/wiki/%E6%98%BE%E8%91%97%E6%80%A7%E5%B7%AE%E5%BC%82 "显著性差异")的[二项试验](https://zh.wikipedia.org/w/index.php?title=%E4%BA%8C%E9%A1%B9%E8%AF%95%E9%AA%8C&action=edit&redlink=1 "二项试验（页面不存在）")的基础。

二项分布包括下列条件

1. 你正在进行一系列独立试验
2. 每一次试验都存在失败和成功的可能，每一次试验的成功概率相同(前两个条件和几何分布的条件相同)
3. 试验次数有限

二项分布的分布函数如下：

$$
P(X=r)=C^n_rp^rq^{1-r}
$$

其中，

$$
C^r_n=\frac{n!}{r!(n-r)!}
$$

 $p​$ 是一次试验的成功概率，n是试验次数，服从二项分布写作

$$
X\sim{B(n,p)}
$$

### 二项分布的期望与方差

推导：假设每个$X_i$ 是一次单独的试验

服从二项分布的试验次数为*n*， 由于每次试验之间独立，所以有$E(X_1)=E(X_2)=...E(X_n)$ 

且$Var(X)=Var(X_1)+Var(X_2)+...+Var(X_n)$ ，

$Var(X)=E(X^2)+E(X)^2=(0q+1p)-p^2=p(1-p)=pq​$

所以：

$$
\begin{aligned}
E(X)&=E(X_1)+E(X_2)+...+E(X_n)\\\
&=nE(X_i)\\\
&=np\\\
\end{aligned}
$$

$$
\begin{aligned}
Var(X)&=Var(X_1)+Var(X_2)+...+Var(X_n)\\\
&=nVar(X_i)\\\
&=npq\\\
\end{aligned}
$$

## 泊松分布

### 泊松分布定义

**泊松分布**（法语：loi de Poisson；英语：Poisson distribution）又称**Poisson分布**、**帕松分布**、**布瓦松分布**、**布阿松分布**、**普阿松分布**、**波以松分布**、**卜氏分布**、**帕松小数法则**（Poisson law of small numbers），是一种[统计](https://zh.wikipedia.org/wiki/%E7%B5%B1%E8%A8%88 "统计")与[概率](https://zh.wikipedia.org/wiki/%E6%A6%82%E7%8E%87 "概率")学里常见到的[离散概率分布](https://zh.wikipedia.org/wiki/%E6%A9%9F%E7%8E%87%E5%88%86%E4%BD%88 "概率分布")，由法国数学家[西莫恩·德尼·泊松](https://zh.wikipedia.org/wiki/%E8%A5%BF%E8%8E%AB%E6%81%A9%C2%B7%E4%B8%B9%E5%B0%BC%C2%B7%E6%B3%8A%E6%9D%BE "西莫恩·丹尼·泊松")在1838年时发表。

泊松分布适合于描述单位时间内随机事件发生的次数的概率分布。如某一服务设施在一定时间内受到的服务请求的次数，电话交换机接到呼叫的次数、汽车站台的候客人数、机器出现的故障数、自然灾害发生的次数、DNA序列的变异数、放射性原子核的衰变数、激光的光子数分布等等。

1. 单独事件在给定的区间内随机、独立地发生，给定区间可以是时间或空间，例如可以是一个星期，也可以是一英里
2. 已知该区间内的事件平均发生次数(或者叫发生率)，且为有限数值。该事件平均发生次数通常用希腊字母$\lambda​$ 表示

让我们用X表示给定区间内时间的发生次数，例如一个星期内的损坏次数。如果X符合泊松分布，且每个区间内平均发生$\lambda$ 次，或者说发生率为$\lambda$ ， 则写作

$$
X\sim{Po(\lambda)}
$$

概率计算如下：

$$
P(X=r)=\frac{e^{-\lambda}\lambda^r}{r!}
$$

### 泊松分布的期望与方差

很简洁：

$$
E(X)=\lambda,Var(X)=\lambda
$$

实际上，泊松分布可以看成是二项分布的极限得到，记常数$ \lambda=np​$

则有如下：

$$
\begin{aligned}
\lim_{n\to\infty,p\to0}C_n^kp^k(1-p)^k&=\lim_{n\to\infty,p\to0}\frac{n(n-1)...(n+1-k)}{k!}p^k(1-p)^{n-k}\\\
&=\lim_{n\to\infty,p\to0}\frac{n^k}{k!}p^k(1-p)^{\frac{\lambda}{p}-k}\\\
&=\lim_{n\to\infty,p\to0}\frac{\lambda^k}{k!}\left[\left(1-p\right)^{\frac{1}{-p}}\right]^{-\lambda}\frac{1}{(1-p)^k}\\\
&=\lim_{n\to\infty,p\to0}\frac{\lambda^k}{k!}e^{-\lambda}
\end{aligned}
$$

泊松分布形状随着$\lambda$ 的数值发生变化，$\lambda$ 越小，则分布越向右倾斜，随着$\lambda$ 变大，分布逐渐变得对称

如果$\lambda​$ 是一个整数，则有两个众数，$\lambda​$ 和 $ \lambda-1​$ ，如果$\lambda ​$ 不是整数，则众数为$\lambda​$ 

泊松分布随机变量的组合

$$
X+Y\sim{Po(\lambda_x+\lambda_y)}
$$

当 $n$ 很大且 $p$ 很小时，可以用 $X\sim{Po(np)}$ 近似代替 $X \sim{B(n,p)}$ ,这样可以方便计算

## 总结

<img src="https://i0.hdslb.com/bfs/album/8569563e510559044f6b1289e2b9cc3c779e6711.png" title="" alt="" data-align="center">
